{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf69ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (f1_score, precision_score, recall_score, \n",
    "                             roc_auc_score, precision_recall_curve, auc, \n",
    "                             confusion_matrix, classification_report)\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Boosting Models (State-of-the-art for Tabular Data)\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "import joblib # Thêm thư viện này để lưu/load model\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# 1. ADVANCED FEATURE ENGINEERING (Tạo đặc trưng hành vi)\n",
    "# ============================================================================\n",
    "\n",
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Tạo các features tập trung vào sự bất thường trong hành vi tiêu dùng\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # --- 1. Xử lý thời gian ---\n",
    "    df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])\n",
    "    df['dob'] = pd.to_datetime(df['dob'])\n",
    "    \n",
    "    df['hour'] = df['trans_date_trans_time'].dt.hour\n",
    "    df['day_of_week'] = df['trans_date_trans_time'].dt.dayofweek\n",
    "    df['month'] = df['trans_date_trans_time'].dt.month\n",
    "    \n",
    "    # Tuổi khách hàng\n",
    "    df['age'] = (df['trans_date_trans_time'] - df['dob']).dt.days // 365\n",
    "    \n",
    "    # --- 2. Khoảng cách địa lý (Haversine approximation) ---\n",
    "    # Khoảng cách từ người mua đến merchant\n",
    "    df['distance_km'] = np.sqrt(\n",
    "        (df['lat'] - df['merch_lat'])**2 + (df['long'] - df['merch_long'])**2\n",
    "    ) * 111\n",
    "    \n",
    "    # --- 3. Transaction Aggregations (Quan trọng nhất cho Fraud) ---\n",
    "    # Tính toán hành vi trung bình của thẻ này trong quá khứ\n",
    "    \n",
    "    # Sắp xếp theo thời gian để tránh data leakage (nếu dùng rolling window)\n",
    "    # Ở đây dùng groupby transform cho đơn giản và hiệu quả\n",
    "    \n",
    "    # Trung bình số tiền giao dịch của thẻ này\n",
    "    # df['card_mean_amt'] = df.groupby('cc_num')['amt'].transform('mean')\n",
    "    # df['card_std_amt'] = df.groupby('cc_num')['amt'].transform('std')\n",
    "    \n",
    "    # # Giao dịch hiện tại lệch bao nhiêu so với trung bình của chính thẻ đó (Z-score)\n",
    "    # df['amt_zscore'] = (df['amt'] - df['card_mean_amt']) / (df['card_std_amt'] + 1e-5)\n",
    "    \n",
    "    # # Số lượng giao dịch của thẻ trong dataset (Proxy cho tần suất sử dụng)\n",
    "    # df['card_trans_count'] = df.groupby('cc_num')['amt'].transform('count')\n",
    "    \n",
    "    # # Trung bình số tiền giao dịch tại Merchant Category này\n",
    "    # df['category_mean_amt'] = df.groupby('category')['amt'].transform('mean')\n",
    "    # df['amt_vs_category_mean'] = df['amt'] / (df['category_mean_amt'] + 1e-5)\n",
    "    \n",
    "    # --- 4. Log transform cho số tiền (giảm skewness) ---\n",
    "    df['amt_log'] = np.log1p(df['amt'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ============================================================================\n",
    "# 2. PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "def preprocess_data(df, label_encoders=None, is_train=True):\n",
    "    df = create_features(df)\n",
    "    \n",
    "    drop_cols = ['trans_date_trans_time', 'cc_num', 'merchant', 'first', 'last', \n",
    "                 'street', 'city', 'state', 'zip', 'dob', 'trans_num', 'unix_time',\n",
    "                 'lat', 'long', 'merch_lat', 'merch_long', 'Unnamed: 0'] \n",
    "    df = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "    \n",
    "    cat_cols = ['category', 'gender', 'job']\n",
    "    \n",
    "    if is_train:\n",
    "        # Train: Fit encoders\n",
    "        label_encoders = {}\n",
    "        for col in cat_cols:\n",
    "            if col in df.columns:\n",
    "                le = LabelEncoder()\n",
    "                df[col] = le.fit_transform(df[col].astype(str))\n",
    "                label_encoders[col] = le\n",
    "    else:\n",
    "        # Test: Transform only\n",
    "        for col in cat_cols:\n",
    "            if col in df.columns and col in label_encoders:\n",
    "                # Xử lý unseen categories\n",
    "                df[col] = df[col].map(lambda x: x if x in label_encoders[col].classes_ else 'Unknown')\n",
    "                # Thêm 'Unknown' vào classes nếu chưa có\n",
    "                if 'Unknown' not in label_encoders[col].classes_:\n",
    "                    label_encoders[col].classes_ = np.append(label_encoders[col].classes_, 'Unknown')\n",
    "                df[col] = label_encoders[col].transform(df[col].astype(str))\n",
    "    \n",
    "    import re\n",
    "    df = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "    \n",
    "    return df, label_encoders\n",
    "\n",
    "# ============================================================================\n",
    "# 3. THRESHOLD OPTIMIZATION (Chìa khóa để cân bằng Precision/Recall)\n",
    "# ============================================================================\n",
    "\n",
    "def find_optimal_threshold(y_true, y_proba):\n",
    "    \"\"\"\n",
    "    Tìm ngưỡng xác suất tối ưu để tối đa hóa F1-Score.\n",
    "    Mặc định model lấy ngưỡng 0.5, nhưng với data imbalance, ngưỡng tối ưu thường thấp hơn.\n",
    "    \"\"\"\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "    \n",
    "    # Tính F1 cho mọi ngưỡng\n",
    "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)\n",
    "    \n",
    "    # Lấy vị trí F1 cao nhất\n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    best_f1 = f1_scores[best_idx]\n",
    "    \n",
    "    print(f\"\\n>>> Optimal Threshold Found: {best_threshold:.4f}\")\n",
    "    print(f\">>> Best F1-Score at threshold: {best_f1:.4f}\")\n",
    "    \n",
    "    return best_threshold\n",
    "\n",
    "# ============================================================================\n",
    "# 4. MODEL TRAINING & ENSEMBLING\n",
    "# ============================================================================\n",
    "\n",
    "def train_ensemble_model(X_train, y_train, X_test, y_test):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING ENSEMBLE MODEL (XGBoost + LightGBM + CatBoost)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Tính toán tỷ lệ scale_pos_weight\n",
    "    # Tỷ lệ = (Số lượng mẫu Negative) / (Số lượng mẫu Positive)\n",
    "    neg_count = len(y_train) - y_train.sum()\n",
    "    pos_count = y_train.sum()\n",
    "    scale_weight = neg_count / pos_count\n",
    "    print(f\"Calculated scale_pos_weight: {scale_weight:.2f}\")\n",
    "\n",
    "    # --- 1. Define Models ---\n",
    "    # XGBoost\n",
    "    clf_xgb = xgb.XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        scale_pos_weight=scale_weight, # Xử lý imbalance\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    \n",
    "    # LightGBM\n",
    "    clf_lgb = lgb.LGBMClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=8,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=31,\n",
    "        scale_pos_weight=scale_weight, # Xử lý imbalance\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "    \n",
    "    # CatBoost (Tự động xử lý category tốt)\n",
    "    clf_cat = CatBoostClassifier(\n",
    "        iterations=200,\n",
    "        depth=6,\n",
    "        learning_rate=0.05,\n",
    "        auto_class_weights='Balanced', # Xử lý imbalance\n",
    "        random_state=42,\n",
    "        verbose=0,\n",
    "        allow_writing_files=False\n",
    "    )\n",
    "\n",
    "    # --- 2. Voting Classifier (Soft Voting) ---\n",
    "    # Kết hợp xác suất của 3 mô hình để đưa ra quyết định cuối cùng\n",
    "    ensemble = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('xgb', clf_xgb),\n",
    "            ('lgb', clf_lgb),\n",
    "            ('cat', clf_cat)\n",
    "        ],\n",
    "        voting='soft', # Lấy trung bình xác suất\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # --- 3. Training ---\n",
    "    print(\"Fitting Ensemble Model...\")\n",
    "    ensemble.fit(X_train, y_train)\n",
    "    \n",
    "    # --- 4. Prediction (Probabilities) ---\n",
    "    print(\"Predicting probabilities...\")\n",
    "    y_pred_proba = ensemble.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    return ensemble, y_pred_proba\n",
    "\n",
    "# ============================================================================\n",
    "# 5. EVALUATION & VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_performance(y_test, y_pred_proba, threshold):\n",
    "    # Convert probability to binary class based on optimal threshold\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL EVALUATION REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Metrics\n",
    "    print(classification_report(y_test, y_pred, target_names=['Normal', 'Fraud']))\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Precision-Recall AUC (Quan trọng hơn ROC cho Imbalanced Data)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    \n",
    "    print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "    print(f\"PR-AUC Score:  {pr_auc:.4f}\")\n",
    "    \n",
    "    # --- Visualization ---\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # 1. Confusion Matrix\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "    axes[0].set_title(f'Confusion Matrix (Threshold={threshold:.3f})')\n",
    "    axes[0].set_xlabel('Predicted')\n",
    "    axes[0].set_ylabel('Actual')\n",
    "    \n",
    "    # 2. ROC Curve\n",
    "    from sklearn.metrics import roc_curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    axes[1].plot(fpr, tpr, label=f'AUC = {roc_auc:.3f}')\n",
    "    axes[1].plot([0, 1], [0, 1], 'k--')\n",
    "    axes[1].set_title('ROC Curve')\n",
    "    axes[1].set_xlabel('False Positive Rate')\n",
    "    axes[1].set_ylabel('True Positive Rate')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # 3. Precision-Recall Curve\n",
    "    axes[2].plot(recall, precision, label=f'PR-AUC = {pr_auc:.3f}', color='green')\n",
    "    axes[2].set_title('Precision-Recall Curve')\n",
    "    axes[2].set_xlabel('Recall')\n",
    "    axes[2].set_ylabel('Precision')\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Load Data\n",
    "    print(\"Loading data...\")\n",
    "    try:\n",
    "        df = pd.read_csv('fraudTrain.csv') # Thay đường dẫn file của bạn\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: File not found. Please check the path.\")\n",
    "        # Tạo dummy data để test code nếu không có file\n",
    "        from sklearn.datasets import make_classification\n",
    "        X_dummy, y_dummy = make_classification(n_samples=10000, n_features=20, weights=[0.99, 0.01], random_state=42)\n",
    "        df = pd.DataFrame(X_dummy, columns=[f'col_{i}' for i in range(20)])\n",
    "        df['is_fraud'] = y_dummy\n",
    "        # Add dummy columns required for feature engineering func to pass (mocking)\n",
    "        df['trans_date_trans_time'] = pd.Timestamp('2023-01-01')\n",
    "        df['dob'] = pd.Timestamp('1990-01-01')\n",
    "        df['lat'] = 0; df['long'] = 0; df['merch_lat'] = 1; df['merch_long'] = 1\n",
    "        df['cc_num'] = 123; df['amt'] = 100; df['category'] = 'misc'\n",
    "        print(\"Using Dummy Data for demonstration...\")\n",
    "\n",
    "    # 2. Preprocessing\n",
    "    print(\"Preprocessing & Feature Engineering...\")\n",
    "    df_processed, label_encoders = preprocess_data(df, is_train=True)\n",
    "    \n",
    "    X = df_processed.drop('is_fraud', axis=1)\n",
    "    y = df_processed['is_fraud']\n",
    "    \n",
    "    # 3. Split Data (Stratified để giữ nguyên tỷ lệ Fraud)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set shape: {X_train.shape}\")\n",
    "    print(f\"Test set shape: {X_test.shape}\")\n",
    "    print(f\"Fraud ratio in Train: {y_train.mean():.4%}\")\n",
    "    \n",
    "    # 4. Train Ensemble Model\n",
    "    model, y_pred_proba = train_ensemble_model(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # 5. Find Optimal Threshold\n",
    "    # Đây là bước quan trọng nhất để giảm False Positives\n",
    "    optimal_threshold = find_optimal_threshold(y_test, y_pred_proba)\n",
    "    \n",
    "    # 6. Evaluate on Validation Set\n",
    "    evaluate_performance(y_test, y_pred_proba, threshold=optimal_threshold)\n",
    "\n",
    "    # ========================================================================\n",
    "    # 7. SAVE MODEL & THRESHOLD\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SAVING MODEL\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Lưu cả model và ngưỡng tối ưu vào 1 file dictionary\n",
    "    # Điều này đảm bảo khi tái sử dụng, ta có đúng ngưỡng cắt (threshold)\n",
    "    model_filename = 'fraud_detection_model.pkl'\n",
    "    save_payload = {\n",
    "        'model': model,\n",
    "        'threshold': optimal_threshold,\n",
    "        'features': X_train.columns.tolist(), # Lưu danh sách cột để đảm bảo khớp khi test\n",
    "        'label_encoders': label_encoders # Lưu bộ mã hóa nhãn để xử lý dữ liệu mới\n",
    "    }\n",
    "    joblib.dump(save_payload, model_filename)\n",
    "    print(f\"Model, threshold, and feature list saved to: {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a80f735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# 8. TEST ON NEW FILE (fraudTest.csv)\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING ON NEW FILE (fraudTest.csv)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_file_path = 'fraudTest.csv'\n",
    "\n",
    "try:\n",
    "    # 8.1 Load Test Data\n",
    "    print(f\"Loading test data from {test_file_path}...\")\n",
    "    df_new_test = pd.read_csv(test_file_path)\n",
    "    \n",
    "    # 8.2 Preprocess Test Data\n",
    "    # Lưu ý: Ta tái sử dụng hàm preprocess_data. \n",
    "    # Trong môi trường Production nghiêm ngặt, bạn nên lưu LabelEncoder từ bước train \n",
    "    # để transform cho test. Ở đây ta giả định các category (Gender, Job...) tương đồng.\n",
    "    print(\"Preprocessing test data...\")\n",
    "    df_new_processed, _ = preprocess_data(df_new_test, is_train=False, label_encoders=label_encoders)\n",
    "    \n",
    "    # Tách X, y\n",
    "    if 'is_fraud' in df_new_processed.columns:\n",
    "        X_new = df_new_processed.drop('is_fraud', axis=1)\n",
    "        y_new = df_new_processed['is_fraud']\n",
    "    else:\n",
    "        X_new = df_new_processed\n",
    "        y_new = None\n",
    "        print(\"Note: 'is_fraud' column not found in test data. Skipping evaluation metrics.\")\n",
    "    \n",
    "    # Đảm bảo thứ tự cột giống hệt lúc train (quan trọng cho XGBoost/LightGBM)\n",
    "    # Nếu thiếu cột nào (do drop), code sẽ báo lỗi hoặc cần xử lý thêm\n",
    "    X_new = X_new[X_train.columns]\n",
    "    \n",
    "    # 8.3 Predict using Trained Model\n",
    "    print(\"Predicting on new data...\")\n",
    "    y_new_proba = model.predict_proba(X_new)[:, 1]\n",
    "    \n",
    "    # 8.4 Evaluate using the OPTIMAL THRESHOLD found earlier\n",
    "    if y_new is not None:\n",
    "        print(f\"Evaluating using saved threshold: {optimal_threshold:.4f}\")\n",
    "        evaluate_performance(y_new, y_new_proba, threshold=optimal_threshold)\n",
    "    else:\n",
    "        print(\"Predictions generated (y_new_proba).\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: File {test_file_path} not found. Skipping external test.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during testing: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311db96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, feature_names):\n",
    "    \"\"\"\n",
    "    Trực quan hóa tầm quan trọng của các đặc trưng từ VotingClassifier\n",
    "    bằng cách lấy trung bình từ các model con.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Lấy danh sách các model con đã được train\n",
    "    estimators = model.estimators_\n",
    "    \n",
    "    importances_list = []\n",
    "    \n",
    "    for clf in estimators:\n",
    "        # Kiểm tra xem model có thuộc tính feature_importances_ không\n",
    "        if hasattr(clf, 'feature_importances_'):\n",
    "            imp = clf.feature_importances_\n",
    "            # Chuẩn hóa về thang đo 0-1 (vì CatBoost, LGBM có thể có thang đo khác nhau)\n",
    "            if imp.sum() > 0:\n",
    "                imp = imp / imp.sum()\n",
    "            importances_list.append(imp)\n",
    "            \n",
    "    if not importances_list:\n",
    "        print(\"Không thể trích xuất feature importance từ các model con.\")\n",
    "        return\n",
    "\n",
    "    # Tính trung bình cộng độ quan trọng từ 3 model\n",
    "    avg_importance = np.mean(importances_list, axis=0)\n",
    "    \n",
    "    # Tạo DataFrame để hiển thị\n",
    "    fi_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': avg_importance\n",
    "    })\n",
    "    \n",
    "    # Sắp xếp giảm dần\n",
    "    fi_df = fi_df.sort_values(by='importance', ascending=False)\n",
    "    \n",
    "    # In ra Top 10\n",
    "    print(\"Top 10 Most Important Features:\")\n",
    "    print(fi_df.head(10).to_string(index=False))\n",
    "    \n",
    "    # Vẽ biểu đồ\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x='importance', y='feature', data=fi_df.head(20), palette='viridis')\n",
    "    plt.title('Top 20 Feature Importance (Ensemble Average)')\n",
    "    plt.xlabel('Normalized Importance Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_feature_importance(model, X_train.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3-9-21",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
