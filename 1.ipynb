{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f4d54f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                             roc_auc_score, precision_recall_curve, auc,\n",
    "                             f1_score, precision_score, recall_score)\n",
    "from sklearn.feature_selection import (SelectKBest, f_classif, mutual_info_classif,\n",
    "                                       RFE, RFECV)\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Imbalanced-learn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "# ============================================================================\n",
    "# 1. COMPREHENSIVE FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "def comprehensive_feature_engineering(df):\n",
    "    \"\"\"Create extensive features without dropping original columns\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE FEATURE ENGINEERING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # ========== TIME FEATURES ==========\n",
    "    df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])\n",
    "    \n",
    "    # Basic time features\n",
    "    df['trans_year'] = df['trans_date_trans_time'].dt.year\n",
    "    df['trans_month'] = df['trans_date_trans_time'].dt.month\n",
    "    df['trans_day'] = df['trans_date_trans_time'].dt.day\n",
    "    df['trans_hour'] = df['trans_date_trans_time'].dt.hour\n",
    "    df['trans_minute'] = df['trans_date_trans_time'].dt.minute\n",
    "    df['trans_dayofweek'] = df['trans_date_trans_time'].dt.dayofweek\n",
    "    df['trans_is_weekend'] = df['trans_dayofweek'].isin([5, 6]).astype(int)\n",
    "    df['trans_is_night'] = df['trans_hour'].isin(range(0, 6)).astype(int)\n",
    "    df['trans_is_business_hours'] = df['trans_hour'].isin(range(9, 18)).astype(int)\n",
    "    \n",
    "    # Cyclical encoding for time\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['trans_hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['trans_hour'] / 24)\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['trans_dayofweek'] / 7)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['trans_dayofweek'] / 7)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['trans_month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['trans_month'] / 12)\n",
    "    \n",
    "    print(f\"✓ Time features created: {df.filter(regex='trans_|hour_|day_|month_').shape[1]} features\")\n",
    "    \n",
    "    # ========== AGE FEATURES ==========\n",
    "    df['dob'] = pd.to_datetime(df['dob'])\n",
    "    df['age'] = (df['trans_date_trans_time'] - df['dob']).dt.days / 365.25\n",
    "    df['age_group'] = pd.cut(df['age'], bins=[0, 25, 35, 45, 55, 65, 100], \n",
    "                              labels=['18-25', '26-35', '36-45', '46-55', '56-65', '65+'])\n",
    "    df['age_group_encoded'] = df['age_group'].cat.codes\n",
    "    \n",
    "    print(f\"✓ Age features created: age, age_group\")\n",
    "    \n",
    "    # ========== GEOGRAPHIC FEATURES ==========\n",
    "    # Distance between customer and merchant\n",
    "    df['distance_km'] = np.sqrt(\n",
    "        (df['lat'] - df['merch_lat'])**2 + \n",
    "        (df['long'] - df['merch_long'])**2\n",
    "    ) * 111  # Approximate km per degree\n",
    "    \n",
    "    # Distance categories\n",
    "    df['distance_category'] = pd.cut(df['distance_km'], \n",
    "                                      bins=[0, 10, 50, 100, 500, 10000],\n",
    "                                      labels=['very_close', 'close', 'medium', 'far', 'very_far'])\n",
    "    df['distance_category_encoded'] = df['distance_category'].cat.codes\n",
    "    \n",
    "    # State interaction (customer state vs merchant location)\n",
    "    df['same_state_area'] = (\n",
    "        (df['lat'] - df['merch_lat']).abs() < 1\n",
    "    ).astype(int)\n",
    "    \n",
    "    print(f\"✓ Geographic features created: distance_km, distance_category, same_state_area\")\n",
    "    \n",
    "    # ========== AMOUNT FEATURES ==========\n",
    "    df['amt_log'] = np.log1p(df['amt'])\n",
    "    df['amt_rounded'] = (df['amt'] % 1 == 0).astype(int)  # Rounded amounts may be suspicious\n",
    "    \n",
    "    # Amount categories\n",
    "    df['amt_category'] = pd.cut(df['amt'], \n",
    "                                bins=[0, 10, 50, 100, 500, 10000],\n",
    "                                labels=['very_small', 'small', 'medium', 'large', 'very_large'])\n",
    "    df['amt_category_encoded'] = df['amt_category'].cat.codes\n",
    "    \n",
    "    print(f\"✓ Amount features created: amt_log, amt_rounded, amt_category\")\n",
    "    \n",
    "    # ========== CATEGORICAL ENCODINGS ==========\n",
    "    # Target encoding for high-cardinality features\n",
    "    categorical_features = ['merchant', 'category', 'state', 'job', 'gender']\n",
    "    \n",
    "    # Label encoding\n",
    "    le_dict = {}\n",
    "    for col in categorical_features:\n",
    "        if col in df.columns:\n",
    "            le = LabelEncoder()\n",
    "            df[f'{col}_encoded'] = le.fit_transform(df[col].astype(str))\n",
    "            le_dict[col] = le\n",
    "    \n",
    "    # Frequency encoding for merchant (high cardinality)\n",
    "    if 'merchant' in df.columns:\n",
    "        merchant_freq = df['merchant'].value_counts(normalize=True).to_dict()\n",
    "        df['merchant_frequency'] = df['merchant'].map(merchant_freq)\n",
    "    \n",
    "    # Category risk score (if target is available)\n",
    "    if 'is_fraud' in df.columns:\n",
    "        category_fraud_rate = df.groupby('category')['is_fraud'].mean().to_dict()\n",
    "        df['category_fraud_rate'] = df['category'].map(category_fraud_rate)\n",
    "        \n",
    "        state_fraud_rate = df.groupby('state')['is_fraud'].mean().to_dict()\n",
    "        df['state_fraud_rate'] = df['state'].map(state_fraud_rate)\n",
    "        \n",
    "        job_fraud_rate = df.groupby('job')['is_fraud'].mean().to_dict()\n",
    "        df['job_fraud_rate'] = df['job'].map(job_fraud_rate)\n",
    "    \n",
    "    print(f\"✓ Categorical encodings created for: {categorical_features}\")\n",
    "    \n",
    "    # ========== CARD-BASED FEATURES (aggregation) ==========\n",
    "    df = df.sort_values(['cc_num', 'trans_date_trans_time'])\n",
    "    \n",
    "    # Time since last transaction\n",
    "    df['time_since_last_trans'] = df.groupby('cc_num')['unix_time'].diff()\n",
    "    df['time_since_last_trans'].fillna(df['time_since_last_trans'].median(), inplace=True)\n",
    "    \n",
    "    # Transaction velocity features (count in last X hours)\n",
    "    # Simplified version - in production, use rolling windows\n",
    "    df['trans_count_by_card'] = df.groupby('cc_num')['cc_num'].transform('count')\n",
    "    \n",
    "    # Average amount per card\n",
    "    df['avg_amt_by_card'] = df.groupby('cc_num')['amt'].transform('mean')\n",
    "    df['std_amt_by_card'] = df.groupby('cc_num')['amt'].transform('std')\n",
    "    df['amt_deviation'] = (df['amt'] - df['avg_amt_by_card']) / (df['std_amt_by_card'] + 1e-5)\n",
    "    \n",
    "    print(f\"✓ Card-based aggregation features created\")\n",
    "    \n",
    "    # ========== POPULATION FEATURES ==========\n",
    "    df['city_pop_log'] = np.log1p(df['city_pop'])\n",
    "    df['city_pop_category'] = pd.cut(df['city_pop'], \n",
    "                                      bins=[0, 1000, 10000, 100000, 1000000, 10000000],\n",
    "                                      labels=['very_small', 'small', 'medium', 'large', 'very_large'])\n",
    "    df['city_pop_category_encoded'] = df['city_pop_category'].cat.codes\n",
    "    \n",
    "    print(f\"✓ Population features created\")\n",
    "    \n",
    "    # ========== DERIVED FEATURES ==========\n",
    "    # Amount per population ratio\n",
    "    df['amt_per_city_pop'] = df['amt'] / (df['city_pop'] + 1)\n",
    "    \n",
    "    # Distance to amount ratio (high distance + low amount = suspicious)\n",
    "    df['distance_amt_ratio'] = df['distance_km'] / (df['amt'] + 1)\n",
    "    \n",
    "    print(f\"✓ Derived features created\")\n",
    "    \n",
    "    print(f\"\\n✓ Total features after engineering: {df.shape[1]} columns\")\n",
    "    \n",
    "    return df, le_dict\n",
    "\n",
    "# ============================================================================\n",
    "# 2. FEATURE IMPORTANCE ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_feature_importance(X, y, feature_names):\n",
    "    \"\"\"Comprehensive feature importance analysis using multiple methods\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    importance_results = pd.DataFrame({'feature': feature_names})\n",
    "    \n",
    "    # ========== 1. RANDOM FOREST IMPORTANCE ==========\n",
    "    print(\"\\n[1/5] Random Forest Feature Importance...\")\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1,\n",
    "                                class_weight='balanced', max_depth=10)\n",
    "    rf.fit(X, y)\n",
    "    importance_results['rf_importance'] = rf.feature_importances_\n",
    "    \n",
    "    # ========== 2. GRADIENT BOOSTING IMPORTANCE ==========\n",
    "    print(\"[2/5] Gradient Boosting Feature Importance...\")\n",
    "    gb = GradientBoostingClassifier(n_estimators=100, random_state=42, max_depth=5)\n",
    "    gb.fit(X, y)\n",
    "    importance_results['gb_importance'] = gb.feature_importances_\n",
    "    \n",
    "    # ========== 3. XGBOOST IMPORTANCE ==========\n",
    "    print(\"[3/5] XGBoost Feature Importance...\")\n",
    "    xgb_model = xgb.XGBClassifier(scale_pos_weight=260, random_state=42,\n",
    "                                  n_estimators=100, max_depth=6, eval_metric='logloss')\n",
    "    xgb_model.fit(X, y)\n",
    "    importance_results['xgb_importance'] = xgb_model.feature_importances_\n",
    "    \n",
    "    # ========== 4. MUTUAL INFORMATION ==========\n",
    "    print(\"[4/5] Mutual Information...\")\n",
    "    mi_scores = mutual_info_classif(X, y, random_state=42)\n",
    "    importance_results['mutual_info'] = mi_scores\n",
    "    \n",
    "    # ========== 5. F-SCORE (ANOVA) ==========\n",
    "    print(\"[5/5] F-Score (ANOVA)...\")\n",
    "    f_scores, _ = f_classif(X, y)\n",
    "    importance_results['f_score'] = f_scores\n",
    "    \n",
    "    # ========== NORMALIZE AND AGGREGATE ==========\n",
    "    for col in ['rf_importance', 'gb_importance', 'xgb_importance', 'mutual_info', 'f_score']:\n",
    "        importance_results[f'{col}_norm'] = (\n",
    "            importance_results[col] / importance_results[col].max()\n",
    "        )\n",
    "    \n",
    "    # Average importance score\n",
    "    importance_results['avg_importance'] = importance_results[\n",
    "        ['rf_importance_norm', 'gb_importance_norm', 'xgb_importance_norm',\n",
    "         'mutual_info_norm', 'f_score_norm']\n",
    "    ].mean(axis=1)\n",
    "    \n",
    "    # Rank features\n",
    "    importance_results = importance_results.sort_values('avg_importance', ascending=False)\n",
    "    importance_results['rank'] = range(1, len(importance_results) + 1)\n",
    "    \n",
    "    return importance_results\n",
    "\n",
    "def plot_feature_importance(importance_df, top_n=30):\n",
    "    \"\"\"Visualize feature importance\"\"\"\n",
    "    \n",
    "    top_features = importance_df.head(top_n)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Average importance\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.barh(range(len(top_features)), top_features['avg_importance'])\n",
    "    ax1.set_yticks(range(len(top_features)))\n",
    "    ax1.set_yticklabels(top_features['feature'], fontsize=8)\n",
    "    ax1.set_xlabel('Average Normalized Importance')\n",
    "    ax1.set_title(f'Top {top_n} Features - Average Importance')\n",
    "    ax1.invert_yaxis()\n",
    "    ax1.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Random Forest\n",
    "    ax2 = axes[0, 1]\n",
    "    top_rf = importance_df.nlargest(top_n, 'rf_importance')\n",
    "    ax2.barh(range(len(top_rf)), top_rf['rf_importance'], color='green')\n",
    "    ax2.set_yticks(range(len(top_rf)))\n",
    "    ax2.set_yticklabels(top_rf['feature'], fontsize=8)\n",
    "    ax2.set_xlabel('RF Importance')\n",
    "    ax2.set_title('Random Forest Feature Importance')\n",
    "    ax2.invert_yaxis()\n",
    "    ax2.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # XGBoost\n",
    "    ax3 = axes[1, 0]\n",
    "    top_xgb = importance_df.nlargest(top_n, 'xgb_importance')\n",
    "    ax3.barh(range(len(top_xgb)), top_xgb['xgb_importance'], color='orange')\n",
    "    ax3.set_yticks(range(len(top_xgb)))\n",
    "    ax3.set_yticklabels(top_xgb['feature'], fontsize=8)\n",
    "    ax3.set_xlabel('XGB Importance')\n",
    "    ax3.set_title('XGBoost Feature Importance')\n",
    "    ax3.invert_yaxis()\n",
    "    ax3.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Mutual Information\n",
    "    ax4 = axes[1, 1]\n",
    "    top_mi = importance_df.nlargest(top_n, 'mutual_info')\n",
    "    ax4.barh(range(len(top_mi)), top_mi['mutual_info'], color='red')\n",
    "    ax4.set_yticks(range(len(top_mi)))\n",
    "    ax4.set_yticklabels(top_mi['feature'], fontsize=8)\n",
    "    ax4.set_xlabel('Mutual Information')\n",
    "    ax4.set_title('Mutual Information Scores')\n",
    "    ax4.invert_yaxis()\n",
    "    ax4.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 3. CORRELATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_correlations(df, target_col='is_fraud'):\n",
    "    \"\"\"Analyze correlations with target and between features\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CORRELATION ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Select only numeric columns\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    if target_col in numeric_df.columns:\n",
    "        # Correlation with target\n",
    "        target_corr = numeric_df.corr()[target_col].abs().sort_values(ascending=False)\n",
    "        \n",
    "        print(f\"\\nTop 20 features correlated with {target_col}:\")\n",
    "        print(target_corr.head(20))\n",
    "        \n",
    "        # Plot correlation with target\n",
    "        plt.figure(figsize=(10, 12))\n",
    "        top_corr = target_corr.head(30)\n",
    "        plt.barh(range(len(top_corr)), top_corr.values)\n",
    "        plt.yticks(range(len(top_corr)), top_corr.index, fontsize=8)\n",
    "        plt.xlabel('Absolute Correlation with is_fraud')\n",
    "        plt.title('Top 30 Features - Correlation with Target')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.grid(axis='x', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('correlation_with_target.png', dpi=300)\n",
    "        plt.show()\n",
    "        \n",
    "        # Feature correlation matrix (top features only)\n",
    "        top_features = target_corr.head(20).index.tolist()\n",
    "        corr_matrix = numeric_df[top_features].corr()\n",
    "        \n",
    "        plt.figure(figsize=(14, 12))\n",
    "        sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0,\n",
    "                   square=True, linewidths=0.5)\n",
    "        plt.title('Correlation Matrix - Top 20 Features')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('feature_correlation_matrix.png', dpi=300)\n",
    "        plt.show()\n",
    "        \n",
    "        return target_corr, corr_matrix\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "# ============================================================================\n",
    "# 4. COMPARE MODELS WITH DIFFERENT FEATURE SETS\n",
    "# ============================================================================\n",
    "\n",
    "def compare_feature_sets(X_full, y, importance_df):\n",
    "    \"\"\"Compare model performance with different feature sets\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPARING MODELS WITH DIFFERENT FEATURE SETS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Define feature sets\n",
    "    feature_sets = {\n",
    "        'All Features': X_full.columns.tolist(),\n",
    "        'Top 50': importance_df.head(50)['feature'].tolist(),\n",
    "        'Top 30': importance_df.head(30)['feature'].tolist(),\n",
    "        'Top 20': importance_df.head(20)['feature'].tolist(),\n",
    "        'Top 10': importance_df.head(10)['feature'].tolist(),\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for set_name, features in feature_sets.items():\n",
    "        # Filter features that exist in X_full\n",
    "        features = [f for f in features if f in X_full.columns]\n",
    "        X_subset = X_full[features]\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Feature Set: {set_name} ({len(features)} features)\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_subset, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Scale\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Apply SMOTE\n",
    "        smote = SMOTETomek(random_state=42)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "        \n",
    "        # Test multiple models\n",
    "        models = {\n",
    "            'XGBoost': xgb.XGBClassifier(scale_pos_weight=260, n_estimators=100,\n",
    "                                        max_depth=6, random_state=42, eval_metric='logloss'),\n",
    "            'LightGBM': lgb.LGBMClassifier(class_weight='balanced', n_estimators=100,\n",
    "                                          max_depth=6, random_state=42, verbose=-1),\n",
    "            'Random Forest': RandomForestClassifier(class_weight='balanced', n_estimators=100,\n",
    "                                                   max_depth=10, random_state=42, n_jobs=-1),\n",
    "        }\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            print(f\"\\n  Training {model_name}...\")\n",
    "            \n",
    "            model.fit(X_train_resampled, y_train_resampled)\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "            \n",
    "            # Metrics\n",
    "            precision = precision_score(y_test, y_pred)\n",
    "            recall = recall_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            roc_auc = roc_auc_score(y_test, y_proba)\n",
    "            \n",
    "            prec_curve, rec_curve, _ = precision_recall_curve(y_test, y_proba)\n",
    "            pr_auc = auc(rec_curve, prec_curve)\n",
    "            \n",
    "            results.append({\n",
    "                'Feature Set': set_name,\n",
    "                'Num Features': len(features),\n",
    "                'Model': model_name,\n",
    "                'Precision': precision,\n",
    "                'Recall': recall,\n",
    "                'F1-Score': f1,\n",
    "                'ROC-AUC': roc_auc,\n",
    "                'PR-AUC': pr_auc\n",
    "            })\n",
    "            \n",
    "            print(f\"    Precision: {precision:.4f}, Recall: {recall:.4f}, \"\n",
    "                  f\"F1: {f1:.4f}, ROC-AUC: {roc_auc:.4f}\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "def plot_feature_set_comparison(results_df):\n",
    "    \"\"\"Visualize comparison of different feature sets\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    \n",
    "    metrics = ['F1-Score', 'ROC-AUC', 'Precision', 'Recall']\n",
    "    \n",
    "    for idx, metric in enumerate(metrics):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        \n",
    "        pivot_data = results_df.pivot(index='Feature Set', columns='Model', values=metric)\n",
    "        pivot_data.plot(kind='bar', ax=ax, width=0.8)\n",
    "        \n",
    "        ax.set_xlabel('Feature Set')\n",
    "        ax.set_ylabel(metric)\n",
    "        ax.set_title(f'{metric} by Feature Set and Model')\n",
    "        ax.legend(title='Model', loc='best', fontsize=8)\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_set_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 5. MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution with comprehensive analysis\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"FRAUD DETECTION - COMPREHENSIVE FEATURE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. Load data\n",
    "    print(\"\\n[1/8] Loading data...\")\n",
    "    df = pd.read_csv('fraudTrain.csv')\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Fraud rate: {df['is_fraud'].mean() * 100:.3f}%\")\n",
    "    \n",
    "    # 2. Feature engineering\n",
    "    print(\"\\n[2/8] Feature engineering...\")\n",
    "    df_engineered, le_dict = comprehensive_feature_engineering(df)\n",
    "    \n",
    "    # 3. Prepare data\n",
    "    print(\"\\n[3/8] Preparing data...\")\n",
    "    \n",
    "    # Drop only truly unnecessary columns\n",
    "    drop_cols = [\n",
    "        'trans_date_trans_time',  # Already extracted features\n",
    "        'cc_num',                  # Used for aggregation, now can drop\n",
    "        'trans_num',               # Unique identifier, no predictive value\n",
    "        'dob',                     # Already extracted age\n",
    "        'unix_time',              # Already have trans_date_trans_time features\n",
    "        # Keep encoded versions of these:\n",
    "        'merchant', 'first', 'last', 'street', 'city',  # High cardinality, kept encoded versions\n",
    "        'age_group', 'distance_category', 'amt_category', 'city_pop_category'  # Categorical, kept encoded\n",
    "    ]\n",
    "    \n",
    "    df_model = df_engineered.drop(columns=[col for col in drop_cols if col in df_engineered.columns])\n",
    "    \n",
    "    # Select only numeric columns\n",
    "    numeric_cols = df_model.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'is_fraud' in numeric_cols:\n",
    "        numeric_cols.remove('is_fraud')\n",
    "    \n",
    "    X = df_model[numeric_cols]\n",
    "    y = df_model['is_fraud']\n",
    "    \n",
    "    print(f\"Features for modeling: {X.shape[1]} columns\")\n",
    "    print(f\"Feature names: {list(X.columns)[:10]}... (showing first 10)\")\n",
    "    \n",
    "    # 4. Correlation analysis\n",
    "    print(\"\\n[4/8] Analyzing correlations...\")\n",
    "    target_corr, corr_matrix = analyze_correlations(df_model, 'is_fraud')\n",
    "    \n",
    "    # 5. Feature importance analysis\n",
    "    print(\"\\n[5/8] Analyzing feature importance...\")\n",
    "    importance_df = analyze_feature_importance(X.values, y.values, X.columns.tolist())\n",
    "    \n",
    "    # Save importance results\n",
    "    importance_df.to_csv('feature_importance_results.csv', index=False)\n",
    "    print(\"\\n✓ Feature importance saved to 'feature_importance_results.csv'\")\n",
    "    \n",
    "    # Display top features\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TOP 20 MOST IMPORTANT FEATURES\")\n",
    "    print(\"=\"*80)\n",
    "    print(importance_df.head(20)[['feature', 'avg_importance', 'rank']].to_string(index=False))\n",
    "    \n",
    "    # 6. Visualize importance\n",
    "    print(\"\\n[6/8] Creating visualizations...\")\n",
    "    plot_feature_importance(importance_df, top_n=30)\n",
    "    \n",
    "    # 7. Compare feature sets\n",
    "    print(\"\\n[7/8] Comparing different feature sets...\")\n",
    "    feature_comparison_df = compare_feature_sets(X, y, importance_df)\n",
    "    \n",
    "    # Save comparison results\n",
    "    feature_comparison_df.to_csv('feature_set_comparison_results.csv', index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FEATURE SET COMPARISON RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(feature_comparison_df.to_string(index=False))\n",
    "    \n",
    "    # 8. Visualize feature set comparison\n",
    "    print(\"\\n[8/8] Creating comparison visualizations...\")\n",
    "    plot_feature_set_comparison(feature_comparison_df)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ANALYSIS COMPLETED!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nGenerated files:\")\n",
    "    print(\"  1. feature_importance_results.csv - Detailed feature importance scores\")\n",
    "    print(\"  2. feature_set_comparison_results.csv - Performance with different feature sets\")\n",
    "    print(\"  3. feature_importance_analysis.png - Feature importance visualizations\")\n",
    "    print(\"  4. correlation_with_target.png - Target correlation analysis\")\n",
    "    print(\"  5. feature_correlation_matrix.png - Feature correlation heatmap\")\n",
    "    print(\"  6. feature_set_comparison.png - Feature set performance comparison\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"KEY INSIGHTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Best feature set\n",
    "    best_result = feature_comparison_df.loc[feature_comparison_df['F1-Score'].idxmax()]\n",
    "    print(f\"\\n✓ Best performing configuration:\")\n",
    "    print(f\"  - Feature Set: {best_result['Feature Set']} ({best_result['Num Features']} features)\")\n",
    "    print(f\"  - Model: {best_result['Model']}\")\n",
    "    print(f\"  - F1-Score: {best_result['F1-Score']:.4f}\")\n",
    "    print(f\"  - Precision: {best_result['Precision']:.4f}\")\n",
    "    print(f\"  - Recall: {best_result['Recall']:.4f}\")\n",
    "    \n",
    "    # Feature reduction benefit\n",
    "    all_features_f1 = feature_comparison_df[\n",
    "        feature_comparison_df['Feature Set'] == 'All Features'\n",
    "    ]['F1-Score'].mean()\n",
    "    \n",
    "    top_30_f1 = feature_comparison_df[\n",
    "        feature_comparison_df['Feature Set'] == 'Top 30'\n",
    "    ]['F1-Score'].mean()\n",
    "    \n",
    "    print(f\"\\n✓ Feature reduction analysis:\")\n",
    "    print(f\"  - Average F1 with All Features: {all_features_f1:.4f}\")\n",
    "    print(f\"  - Average F1 with Top 30 Features: {top_30_f1:.4f}\")\n",
    "    print(f\"  - Performance change: {((top_30_f1 - all_features_f1) / all_features_f1 * 100):+.2f}%\")\n",
    "    \n",
    "    return importance_df, feature_comparison_df, df_engineered\n",
    "\n",
    "# ============================================================================\n",
    "# RUN\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    importance_df, comparison_df, df_engineered = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceba52d4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
